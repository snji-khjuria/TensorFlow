{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*2 equals 4.0\n"
     ]
    }
   ],
   "source": [
    "#Program:-1 multiplication program\n",
    "import tensorflow as tf\n",
    "a = tf.placeholder(\"float\")\n",
    "b = tf.placeholder(\"float\")\n",
    "c = tf.multiply(a, b)\n",
    "with tf.Session() as sess:\n",
    "    print(\"2*2 equals \" + str(sess.run(c, feed_dict={a:2, b:2})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.967\n"
     ]
    }
   ],
   "source": [
    "#program2: Linear regression\n",
    "#essential import\n",
    "#whole process\n",
    "#create dataset->create model mapping x to y->create cost y and yhat->create optimizer for cost\n",
    "#          ->run optimizer for dict with my x and y->see the model output\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#create dataset\n",
    "trX = np.linspace(-1, 1, 101)\n",
    "# create a y value which is approximately linear but with some random noise\n",
    "trY = 2 * trX + np.random.randn(*trX.shape) * 0.33 \n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "def model(X, w):\n",
    "    return tf.multiply(X, w)\n",
    "w = tf.Variable(0.0, name=\"weights\")\n",
    "y_model = model(X, w)\n",
    "#square error for cost function\n",
    "cost = tf.square(Y-y_model)\n",
    "#construct optimizer to minimize the cost\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(100):\n",
    "        for x, y in zip(trX, trY):\n",
    "            sess.run(train_op, feed_dict={X:x, Y:y})\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0 index 0.8834\n",
      "1 index 0.8967\n",
      "2 index 0.9037\n",
      "3 index 0.9071\n",
      "4 index 0.9099\n",
      "5 index 0.9109\n",
      "6 index 0.9122\n",
      "7 index 0.9133\n",
      "8 index 0.9151\n",
      "9 index 0.9159\n",
      "10 index 0.9161\n",
      "11 index 0.9165\n",
      "12 index 0.9167\n",
      "13 index 0.9166\n",
      "14 index 0.9171\n",
      "15 index 0.9178\n",
      "16 index 0.9183\n",
      "17 index 0.9188\n",
      "18 index 0.9191\n",
      "19 index 0.9199\n",
      "20 index 0.9198\n",
      "21 index 0.92\n",
      "22 index 0.9205\n",
      "23 index 0.9205\n",
      "24 index 0.9207\n",
      "25 index 0.9206\n",
      "26 index 0.9211\n",
      "27 index 0.9212\n",
      "28 index 0.9215\n",
      "29 index 0.9215\n",
      "30 index 0.9215\n",
      "31 index 0.9214\n",
      "32 index 0.9215\n",
      "33 index 0.9213\n",
      "34 index 0.9212\n",
      "35 index 0.9212\n",
      "36 index 0.9209\n",
      "37 index 0.9213\n",
      "38 index 0.9216\n",
      "39 index 0.9214\n",
      "40 index 0.9215\n",
      "41 index 0.9215\n",
      "42 index 0.9216\n",
      "43 index 0.9219\n",
      "44 index 0.9218\n",
      "45 index 0.9218\n",
      "46 index 0.9219\n",
      "47 index 0.9218\n",
      "48 index 0.9217\n",
      "49 index 0.9216\n",
      "50 index 0.9216\n",
      "51 index 0.9217\n",
      "52 index 0.9214\n",
      "53 index 0.9219\n",
      "54 index 0.9221\n",
      "55 index 0.9224\n",
      "56 index 0.9224\n",
      "57 index 0.9223\n",
      "58 index 0.9223\n",
      "59 index 0.9223\n",
      "60 index 0.9225\n",
      "61 index 0.9225\n",
      "62 index 0.9227\n",
      "63 index 0.923\n",
      "64 index 0.9232\n",
      "65 index 0.9232\n",
      "66 index 0.9232\n",
      "67 index 0.9233\n",
      "68 index 0.9232\n",
      "69 index 0.9231\n",
      "70 index 0.9231\n",
      "71 index 0.9232\n",
      "72 index 0.9233\n",
      "73 index 0.9233\n",
      "74 index 0.9233\n",
      "75 index 0.9234\n",
      "76 index 0.9235\n",
      "77 index 0.9237\n",
      "78 index 0.9237\n",
      "79 index 0.9236\n",
      "80 index 0.9237\n",
      "81 index 0.9236\n",
      "82 index 0.9237\n",
      "83 index 0.9237\n",
      "84 index 0.9237\n",
      "85 index 0.9237\n",
      "86 index 0.9236\n",
      "87 index 0.9236\n",
      "88 index 0.9237\n",
      "89 index 0.9238\n",
      "90 index 0.9237\n",
      "91 index 0.9236\n",
      "92 index 0.9234\n",
      "93 index 0.9233\n",
      "94 index 0.9233\n",
      "95 index 0.9234\n",
      "96 index 0.9234\n",
      "97 index 0.9235\n",
      "98 index 0.9236\n",
      "99 index 0.9235\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "#model is same but cost will be cross entropy and softmax output prediction\n",
    "def model(X, w):\n",
    "    return tf.matmul(X, w)\n",
    "#load the dataset\n",
    "mnist              = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "X                  = tf.placeholder(\"float\", [None, 784])\n",
    "Y                  = tf.placeholder(\"float\", [None, 10])\n",
    "w                  = init_weights([784, 10])\n",
    "pr_y               = model(X, w)\n",
    "#compute mean cross entropy with softmax applied internally\n",
    "cost               = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pr_y, labels=Y)) \n",
    "train_op           = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\n",
    "#1 means which axis to reduce for.\n",
    "predict_op         = tf.argmax(pr_y, 1)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(100):\n",
    "        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\n",
    "            sess.run(train_op, feed_dict={X:trX[start:end], Y:trY[start:end]})\n",
    "        predicts = sess.run(predict_op, feed_dict={X:teX})\n",
    "        trueAnswers = np.argmax(teY, axis=1)\n",
    "        print(str(i) + \" index \" + str(np.mean(trueAnswers==predicts)))\n",
    "#         print(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
